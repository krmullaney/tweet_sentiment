{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MaMNQ0uvcwb",
        "outputId": "45ee63db-a510-4323-d093-2e10f4d2499f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp38-cp38-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from torchtext==0.10.0) (4.64.1)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 11 kB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.12.1+cu113\n",
            "    Uninstalling torch-1.12.1+cu113:\n",
            "      Successfully uninstalled torch-1.12.1+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.13.1\n",
            "    Uninstalling torchtext-0.13.1:\n",
            "      Successfully uninstalled torchtext-0.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.10.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNOfdOLzjqVu"
      },
      "outputs": [],
      "source": [
        "# Import required packages\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import time\n",
        "import spacy\n",
        "\n",
        "from sklearn import metrics\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torchtext.legacy\n",
        "from torchtext.legacy import data\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torch.nn import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXjE5WL3mSxM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SUrmHWemS4r"
      },
      "outputs": [],
      "source": [
        "path_to_utils='/content/drive/My Drive/NLP'\n",
        "sys.path.append(path_to_utils)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0COq13xb9b-"
      },
      "outputs": [],
      "source": [
        "os.chdir(path_to_utils)\n",
        "\n",
        "print(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iS1gTkUnfxVO"
      },
      "outputs": [],
      "source": [
        "#params can act as config file here\n",
        "\n",
        "params={\"Network\": {\"seed\": 1},\n",
        "        \"display_stats_freq\": 200,\n",
        "        \"network_save_freq\": 1,\n",
        "        \"postreply_data_path\": \"./\",\n",
        "        #\"input_data_path\": \"./data/datasets/semeval_message_level/\",\n",
        "        #\"train_file_name\": \"training_data.txt\",\n",
        "        #\"test_file_name\": \"test_data.txt\",\n",
        "        #\"reply_file_name\": \"data_post_reply.csv\",\n",
        "        #\"reply_with_label_file_name\": \"data_post_reply_withlabel.csv\",\n",
        "        \"final_data_post_reply_file_name\": \"final_data_post_reply.csv\",\n",
        "        \"training_post_reply_file_name\": \"obtained_train.csv\", #obtained_train.csv, provided_train.csv, supp_obt_train.csv\n",
        "        #\"philipp_data\": \"philipp_data.csv\",\n",
        "        #\"philipp_with_label_file_name\": \"philipp_withlabel.csv\",\n",
        "        #\"philipp_final_post_reply_file_name\": \"philipp_final.csv\",\n",
        "        \"final_test_post_reply_file_name\": \"test_w_text.csv\",\n",
        "        #\"data_format\": \"tsv\",\n",
        "        \"reply_data_format\": \"csv\",\n",
        "        \"pretrained_embedding\": \"glove.twitter.27B.200d\",\n",
        "        \"tokenizer\": \"spacy\",\n",
        "        \"network_output_path\": \"./models/\",\n",
        "        #\"output_data_path\": \"./data/output_data/\",\n",
        "        #\"tb_logs_path\": \"./data/tensor_board_logs/\",\n",
        "        #\"checkpoint_name\": \"checkpoint.tar\",\n",
        "        \"trained_model_name\": \"CNN_model_obt.pth\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngfurp0uhMhh"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "\n",
        "class Mode(Enum):\n",
        "    '''\n",
        "    Class Enumerating the 3 modes of operation of the network.\n",
        "    This is used while loading datasets\n",
        "    '''\n",
        "    TRAIN = 0\n",
        "    VALID = 1\n",
        "    TEST = 2\n",
        "    PREDICTION = 3\n",
        "    REPLYPREDICTION = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giF66QlEexod"
      },
      "outputs": [],
      "source": [
        "class data_provider_PostReply():\n",
        "    '''\n",
        "    Packed padded sequences\n",
        "    Tokenizer: spacy\n",
        "    '''\n",
        "    def __init__(self, params, batch_size=1, split_ratio=0.8, max_vocab_size=25000, mode=Mode.TRAIN, model_mode='RNN', seed=1): #cfg_path\n",
        "        '''\n",
        "        Args:\n",
        "            cfg_path (string): #deprecated for first pass\n",
        "                Config file path of the experiment\n",
        "\n",
        "            params\n",
        "            max_vocab_size (int):\n",
        "                The number of unique words in our training set is usually over 100,000,\n",
        "                which means that our one-hot vectors will have over 100,000 dimensions! SLOW TRAINIG!\n",
        "                We only take the top max_vocab_size most common words.\n",
        "            split_ratio (float):\n",
        "                train-valid splitting\n",
        "            mode (enumeration Mode):\n",
        "                Nature of operation to be done with the data.\n",
        "                Possible inputs are Mode.PREDICTION, Mode.TRAIN, Mode.VALID, Mode.TEST\n",
        "                Default value: Mode.TRAIN\n",
        "        '''\n",
        "        #params = read_config(cfg_path)\n",
        "        #self.cfg_path = cfg_path\n",
        "        self.mode = mode\n",
        "        self.seed = seed\n",
        "        self.split_ratio = split_ratio\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.dataset_path = params['postreply_data_path']\n",
        "        self.train_file_name = params['training_post_reply_file_name']\n",
        "        self.test_file_name = params['final_test_post_reply_file_name']\n",
        "        self.data_format = params['reply_data_format']\n",
        "        self.pretrained_embedding = params['pretrained_embedding']\n",
        "        self.tokenizer = params['tokenizer']\n",
        "        self.batch_size = batch_size\n",
        "        self.model_mode = model_mode\n",
        "\n",
        "\n",
        "    def data_loader(self):\n",
        "        '''\n",
        "        :include_lengths: Packed padded sequences: will make our RNN only process the non-padded elements of our sequence,\n",
        "            and for any padded element the `output` will be a zero tensor.\n",
        "            Note: padding is done by adding <pad> (not zero!)\n",
        "        :tokenize: the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the spaCy tokenizer.\n",
        "        '''\n",
        "        if self.model_mode == 'RNN':\n",
        "            #Packed padded sequences\n",
        "            TEXT = data.Field(tokenize=self.tokenizer, include_lengths=True)  # For saving the length of sentences\n",
        "        if self.model_mode == 'CNN':\n",
        "            TEXT = data.Field(tokenize=self.tokenizer, batch_first=True)  # batch dimension is the firs dimension here.\n",
        "        LABEL = data.LabelField()\n",
        "\n",
        "        fields = [('label', LABEL), ('id', None), ('text', TEXT)]\n",
        "\n",
        "        train_data, test_data = data.TabularDataset.splits(\n",
        "            path=self.dataset_path,\n",
        "            train=self.train_file_name,\n",
        "            test=self.test_file_name,\n",
        "            format=self.data_format,\n",
        "            fields=fields,\n",
        "            skip_header=True)\n",
        "\n",
        "        #print(train_data)\n",
        "\n",
        "        # validation data\n",
        "        if self.split_ratio == 1:\n",
        "            valid_data = None\n",
        "        else:\n",
        "            train_data, valid_data = train_data.split(random_state=random.seed(self.seed), split_ratio=self.split_ratio)\n",
        "\n",
        "        # create the vocabulary only on the training set!!!\n",
        "        # vectors: instead of having our word embeddings initialized randomly, they are initialized with these pre-trained vectors.\n",
        "        # initialize words in your vocabulary but not in your pre-trained embeddings to Gaussian\n",
        "        TEXT.build_vocab(train_data, max_size=self.max_vocab_size,\n",
        "                         vectors=self.pretrained_embedding, unk_init=torch.Tensor.normal_)\n",
        "        # TEXT.build_vocab(train_data, max_size=self.max_vocab_size)\n",
        "\n",
        "        LABEL.build_vocab(train_data)\n",
        "\n",
        "        labels = LABEL.vocab.itos\n",
        "        vocab_idx = TEXT.vocab.stoi\n",
        "\n",
        "        vocab_size = len(TEXT.vocab)\n",
        "        pretrained_embeddings = TEXT.vocab.vectors\n",
        "\n",
        "        # the indices of the padding token <pad> and <unk> in the vocabulary\n",
        "        PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
        "        UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "        # What do we do with words that appear in examples but we have cut from the vocabulary?\n",
        "        # We replace them with a special unknown or <unk> token.\n",
        "\n",
        "\n",
        "        # for packed padded sequences all of the tensors within a batch need to be sorted by their lengths\n",
        "        if self.split_ratio == 1:\n",
        "            valid_iterator = None\n",
        "            train_iterator, test_iterator = data.BucketIterator.splits((\n",
        "                train_data, test_data), batch_size=self.batch_size,\n",
        "                sort_within_batch=True, sort_key=lambda x: len(x.text))\n",
        "        else:\n",
        "            train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits((\n",
        "                train_data, valid_data, test_data), batch_size=self.batch_size,\n",
        "                sort_within_batch=True, sort_key=lambda x: len(x.text))\n",
        "\n",
        "        # finding the weights of each label\n",
        "        data_for_weight = pd.read_csv(os.path.join(self.dataset_path, self.train_file_name))\n",
        "        pos_counter = 0\n",
        "        neg_counter = 0\n",
        "        neut_counter = 0\n",
        "        for i in range(len(data_for_weight['label'])):\n",
        "            if (data_for_weight['label'][i] == 'positive'):\n",
        "                pos_counter += 1\n",
        "            if (data_for_weight['label'][i] == 'negative'):\n",
        "                neg_counter += 1\n",
        "            if (data_for_weight['label'][i] == 'neutral'):\n",
        "                neut_counter += 1\n",
        "        overall = neut_counter + pos_counter + neg_counter\n",
        "        neut_weight = overall/neut_counter\n",
        "        neg_weight = overall/neg_counter\n",
        "        pos_weight = overall/pos_counter\n",
        "        if labels == ['neutral', 'negative', 'positive']:\n",
        "            weights = torch.Tensor([neut_weight, neg_weight, pos_weight])\n",
        "        elif labels == ['neutral', 'positive', 'negative']:\n",
        "            weights = torch.Tensor([neut_weight, pos_weight, neg_weight])\n",
        "        elif labels == ['negative', 'neutral', 'positive']:\n",
        "            weights = torch.Tensor([neg_weight, neut_weight, pos_weight])\n",
        "        elif labels == ['negative', 'positive', 'neutral']:\n",
        "            weights = torch.Tensor([neg_weight, pos_weight, neut_weight])\n",
        "        elif labels == ['positive', 'negative', 'neutral']:\n",
        "            weights = torch.Tensor([pos_weight, neg_weight, neut_weight])\n",
        "        elif labels == ['positive', 'neutral', 'negative']:\n",
        "            weights = torch.Tensor([pos_weight, neut_weight, neg_weight])\n",
        "\n",
        "        if self.mode == Mode.TEST:\n",
        "            return test_iterator\n",
        "        elif self.mode == Mode.PREDICTION:\n",
        "            return labels, vocab_idx, vocab_size, PAD_IDX, UNK_IDX, pretrained_embeddings, labels\n",
        "        else:\n",
        "            return train_iterator, valid_iterator, vocab_size, PAD_IDX, UNK_IDX, pretrained_embeddings, weights, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc3DXbxSlaAd"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "class Training:\n",
        "    '''\n",
        "    This class represents training process.\n",
        "    '''\n",
        "    def __init__(self, params, num_epochs=10, RESUME=False, model_mode='RNN', torch_seed=None): #cfg_path\n",
        "        '''\n",
        "        :cfg_path (string): path of the experiment config file\n",
        "        :torch_seed (int): Seed used for random generators in PyTorch functions\n",
        "        '''\n",
        "        self.params = params\n",
        "        #self.cfg_path = cfg_path\n",
        "        self.RESUME = RESUME\n",
        "        self.model_mode = model_mode\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        if RESUME == False:\n",
        "            self.model_info = self.params['Network']\n",
        "            self.model_info['seed'] = torch_seed or self.model_info['seed']\n",
        "            self.epoch = 0\n",
        "            self.num_epochs = num_epochs\n",
        "            self.best_loss = float('inf')\n",
        "            if 'trained_time' in self.model_info:\n",
        "                self.raise_training_complete_exception()\n",
        "            self.setup_cuda()\n",
        "            #self.writer = SummaryWriter(log_dir=os.path.join(self.params['tb_logs_path']))\n",
        "\n",
        "\n",
        "    def setup_cuda(self, cuda_device_id=0):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.backends.cudnn.fastest = True\n",
        "            torch.cuda.set_device(cuda_device_id)\n",
        "            self.device = torch.device('cuda')\n",
        "            torch.cuda.manual_seed_all(self.model_info['seed'])\n",
        "            torch.manual_seed(self.model_info['seed'])\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "\n",
        "\n",
        "    def setup_model(self, model, optimiser, optimiser_params, loss_function, weight):\n",
        "\n",
        "        total_param_num = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "        print(f'Total # of model\\'s trainable parameters: {total_param_num:,}')\n",
        "        print('----------------------------------------------------\\n')\n",
        "\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimiser = optimiser(self.model.parameters(), **optimiser_params)\n",
        "        # self.loss_function = loss_function()\n",
        "        self.loss_function = loss_function(weight=weight.to(self.device))\n",
        "\n",
        "        if 'retrain' in self.model_info and self.model_info['retrain']==True:\n",
        "            self.load_pretrained_model()\n",
        "\n",
        "        # Saves the model, optimiser,loss function name for writing to config file\n",
        "        self.model_info['total_param_num'] = total_param_num\n",
        "        self.model_info['optimiser'] = optimiser.__name__\n",
        "        self.model_info['loss_function'] = loss_function.__name__\n",
        "        self.model_info['optimiser_params'] = optimiser_params\n",
        "        self.params['Network']=self.model_info\n",
        "        #write_config(self.params, self.cfg_path,sort_keys=True)\n",
        "\n",
        "\n",
        "    def load_checkpoint(self, model, optimiser, optimiser_params, loss_function, weight):\n",
        "\n",
        "        checkpoint = torch.load(self.params['network_output_path'] + '/' + self.params['checkpoint_name'])\n",
        "        self.device = None\n",
        "        self.model_info = checkpoint['model_info']\n",
        "        self.setup_cuda()\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimiser = optimiser(self.model.parameters(), **optimiser_params)\n",
        "        self.loss_function = loss_function(weight=weight.to(self.device))\n",
        "\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.optimiser.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.epoch = checkpoint['epoch']\n",
        "        self.loss_function = checkpoint['loss']\n",
        "        self.best_loss = checkpoint['best_loss']\n",
        "        #self.writer = SummaryWriter(log_dir=os.path.join(self.params['tb_logs_path']), purge_step=self.epoch + 1)\n",
        "\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "    def execute_training(self, train_loader, valid_loader=None, batch_size=1):\n",
        "        '''\n",
        "        Executes training by running training and validation at each epoch\n",
        "        '''\n",
        "        self.params = params #read_config(self.cfg_path)\n",
        "\n",
        "        total_start_time = time.time()\n",
        "\n",
        "        if self.RESUME == False:\n",
        "            # Checks if already trained\n",
        "            if 'trained_time' in self.model_info:\n",
        "                self.raise_training_complete_exception\n",
        "\n",
        "            self.model_info = self.params['Network']\n",
        "            self.model_info['num_epoch'] = self.num_epochs or self.model_info['num_epoch']\n",
        "\n",
        "        print('Starting time:' + str(datetime.datetime.now()) +'\\n')\n",
        "\n",
        "        for epoch in range(self.num_epochs - self.epoch):\n",
        "            self.epoch += 1\n",
        "            start_time = time.time()\n",
        "\n",
        "            print('Training (intermediate metrics):')\n",
        "            train_loss, train_acc, train_F1, train_recall, train_precision = self.train_epoch(train_loader, batch_size)\n",
        "\n",
        "            if valid_loader:\n",
        "                print('\\nValidation (intermediate metrics):')\n",
        "                valid_loss, valid_acc, valid_F1, valid_recall, valid_precision = self.valid_epoch(valid_loader, batch_size)\n",
        "\n",
        "            end_time = time.time()\n",
        "            epoch_mins, epoch_secs = self.epoch_time(start_time, end_time)\n",
        "            total_mins, total_secs = self.epoch_time(total_start_time, end_time)\n",
        "\n",
        "            # Writes to the tensorboard after number of steps specified.\n",
        "            #if valid_loader:\n",
        "            #    self.calculate_tb_stats(train_loss, train_F1, train_recall, train_precision, train_acc,\n",
        "            #                            valid_loss, valid_F1, valid_recall, valid_precision, valid_acc)\n",
        "            #else:\n",
        "            #    self.calculate_tb_stats(train_loss, train_F1, train_recall, train_precision, train_acc)\n",
        "\n",
        "            # Saving the model\n",
        "            if valid_loader:\n",
        "                if valid_loss < self.best_loss:\n",
        "                    self.best_loss = valid_loss\n",
        "                    torch.save(self.model.state_dict(), self.params['network_output_path'] + '/' + self.params['trained_model_name'])\n",
        "            else:\n",
        "                if train_loss < self.best_loss:\n",
        "                    self.best_loss = train_loss\n",
        "                    torch.save(self.model.state_dict(), self.params['network_output_path'] + '/' + self.params['trained_model_name'])\n",
        "\n",
        "            # saving the model based on epoch, checkpoint\n",
        "            #self.savings()\n",
        "\n",
        "            # Print accuracy, F1, and loss after each epoch\n",
        "            print('\\n---------------------------------------------------------------')\n",
        "            print(f'Epoch: {self.epoch:02} | Epoch Time: {epoch_mins}m {epoch_secs}s | '\n",
        "                  f'Total Time so far: {total_mins}m {total_secs}s')\n",
        "            print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}% | Train F1: {train_F1:.3f}')\n",
        "            if valid_loader:\n",
        "                print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}% |  Val. F1: {valid_F1:.3f}')\n",
        "            print('---------------------------------------------------------------\\n')\n",
        "\n",
        "\n",
        "    def train_epoch(self, train_loader, batch_size):\n",
        "        '''\n",
        "        Train using one single iteration of all messages (epoch) in dataset\n",
        "        '''\n",
        "        print(\"Epoch [{}/{}]\".format(self.epoch, self.model_info['num_epoch']))\n",
        "        self.model.train()\n",
        "        previous_idx = 0\n",
        "\n",
        "        # initializing the loss list\n",
        "        batch_loss = 0\n",
        "        batch_count = 0\n",
        "\n",
        "        # initializing the caches\n",
        "        logits_cache = torch.from_numpy(np.zeros((len(train_loader) * batch_size, 3)))\n",
        "        max_preds_cache = torch.from_numpy(np.zeros((len(train_loader) * batch_size, 1)))\n",
        "        labels_cache = torch.from_numpy(np.zeros(len(train_loader) * batch_size))\n",
        "\n",
        "        for idx, batch in enumerate(train_loader):\n",
        "            if self.model_mode == \"RNN\":\n",
        "                message, message_lengths = batch.text\n",
        "            if self.model_mode == \"CNN\":\n",
        "                message = batch.text\n",
        "            label = batch.label\n",
        "            message = message.long()\n",
        "            label = label.long()\n",
        "            message = message.to(self.device)\n",
        "            label = label.to(self.device)\n",
        "\n",
        "            self.optimiser.zero_grad()\n",
        "\n",
        "            with torch.set_grad_enabled(True):\n",
        "                if self.model_mode == \"RNN\":\n",
        "                    output = self.model(message, message_lengths).squeeze(1)\n",
        "                if self.model_mode == \"CNN\":\n",
        "                    output = self.model(message).squeeze(1)\n",
        "\n",
        "                # Loss\n",
        "                loss = self.loss_function(output, label)\n",
        "                batch_loss += loss.item()\n",
        "                batch_count += 1\n",
        "                max_preds = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "\n",
        "                # saving the logits and labels of this batch\n",
        "                for i, batch_vector in enumerate(max_preds):\n",
        "                    max_preds_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, batch_vector in enumerate(output):\n",
        "                    logits_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, value in enumerate(label):\n",
        "                    labels_cache[idx * batch_size + i] = value\n",
        "\n",
        "                loss.backward()\n",
        "                self.optimiser.step()\n",
        "\n",
        "                # Prints loss statistics after number of steps specified.\n",
        "                if (idx + 1)%self.params['display_stats_freq'] == 0:\n",
        "                    print('Epoch {:02} | Batch {:03}-{:03} | Train loss: {:.3f}'.\n",
        "                          format(self.epoch, previous_idx, idx, batch_loss / batch_count))\n",
        "                    previous_idx = idx + 1\n",
        "                    batch_loss = 0\n",
        "                    batch_count = 0\n",
        "\n",
        "        '''Metrics calculation over the whole set'''\n",
        "        max_preds_cache = max_preds_cache.cpu()\n",
        "        labels_cache = labels_cache.cpu()\n",
        "\n",
        "        # average=None gives individual scores for each class\n",
        "        # here we only care about the average of positive class and negative class\n",
        "        epoch_accuracy = metrics.accuracy_score(labels_cache, max_preds_cache)\n",
        "        # epoch_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # epoch_precision = metrics.precision_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # epoch_recall = metrics.recall_score(labels_cache, max_preds_cache, average='macro')\n",
        "\n",
        "        epoch_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average=None)\n",
        "        epoch_precision = metrics.precision_score(labels_cache, max_preds_cache, average=None)\n",
        "        epoch_recall = metrics.recall_score(labels_cache, max_preds_cache, average=None)\n",
        "        epoch_f1_score = (epoch_f1_score[1] + epoch_f1_score[2]) / 2\n",
        "        epoch_precision = (epoch_precision[1] + epoch_precision[2]) / 2\n",
        "        epoch_recall = (epoch_recall[1] + epoch_recall[2]) / 2\n",
        "        labels_cache = labels_cache.long()\n",
        "        logits_cache = logits_cache.float()\n",
        "\n",
        "        # Loss\n",
        "        loss = self.loss_function(logits_cache.to(self.device), labels_cache.to(self.device))\n",
        "        epoch_loss = loss.item()\n",
        "\n",
        "        return epoch_loss, epoch_accuracy, epoch_f1_score, epoch_precision, epoch_recall\n",
        "\n",
        "\n",
        "    def valid_epoch(self, valid_loader, batch_size):\n",
        "        '''Test (validation) model after an epoch and calculate loss on valid dataset'''\n",
        "        print(\"Epoch [{}/{}]\".format(self.epoch, self.model_info['num_epoch']))\n",
        "        self.model.eval()\n",
        "        previous_idx = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # initializing the loss list\n",
        "            batch_loss = 0\n",
        "            batch_count = 0\n",
        "\n",
        "            # initializing the caches\n",
        "            logits_cache = torch.from_numpy(np.zeros((len(valid_loader) * batch_size, 3)))\n",
        "            max_preds_cache = torch.from_numpy(np.zeros((len(valid_loader) * batch_size, 1)))\n",
        "            labels_cache = torch.from_numpy(np.zeros(len(valid_loader) * batch_size))\n",
        "\n",
        "            for idx, batch in enumerate(valid_loader):\n",
        "                if self.model_mode == \"RNN\":\n",
        "                    message, message_lengths = batch.text\n",
        "                if self.model_mode == \"CNN\":\n",
        "                    message = batch.text\n",
        "                label = batch.label\n",
        "                message = message.long()\n",
        "                label = label.long()\n",
        "                message = message.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                if self.model_mode == \"RNN\":\n",
        "                    output = self.model(message, message_lengths).squeeze(1)\n",
        "                if self.model_mode == \"CNN\":\n",
        "                    output = self.model(message).squeeze(1)\n",
        "\n",
        "                # Loss\n",
        "                loss = self.loss_function(output, label)\n",
        "                batch_loss += loss.item()\n",
        "                batch_count += 1\n",
        "                max_preds = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "\n",
        "                # saving the logits and labels of this batch\n",
        "                for i, batch_vector in enumerate(max_preds):\n",
        "                    max_preds_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, batch_vector in enumerate(output):\n",
        "                    logits_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, value in enumerate(label):\n",
        "                    labels_cache[idx * batch_size + i] = value\n",
        "\n",
        "                # Prints loss statistics after number of steps specified.\n",
        "                if (idx + 1)%self.params['display_stats_freq'] == 0:\n",
        "                    print('Epoch {:02} | Batch {:03}-{:03} | Val. loss: {:.3f}'.\n",
        "                          format(self.epoch, previous_idx, idx, batch_loss / batch_count))\n",
        "                    previous_idx = idx + 1\n",
        "                    batch_loss = 0\n",
        "                    batch_count = 0\n",
        "\n",
        "        '''Metrics calculation over the whole set'''\n",
        "        max_preds_cache = max_preds_cache.cpu()\n",
        "        labels_cache = labels_cache.cpu()\n",
        "\n",
        "        epoch_accuracy = metrics.accuracy_score(labels_cache, max_preds_cache)\n",
        "        # epoch_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # epoch_precision = metrics.precision_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # epoch_recall = metrics.recall_score(labels_cache, max_preds_cache, average='macro')\n",
        "\n",
        "        epoch_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average=None)\n",
        "        epoch_precision = metrics.precision_score(labels_cache, max_preds_cache, average=None)\n",
        "        epoch_recall = metrics.recall_score(labels_cache, max_preds_cache, average=None)\n",
        "        epoch_f1_score = (epoch_f1_score[1] + epoch_f1_score[2]) / 2\n",
        "        epoch_precision = (epoch_precision[1] + epoch_precision[2]) / 2\n",
        "        epoch_recall = (epoch_recall[1] + epoch_recall[2]) / 2\n",
        "        labels_cache = labels_cache.long()\n",
        "        logits_cache = logits_cache.float()\n",
        "\n",
        "        # Loss\n",
        "        loss = self.loss_function(logits_cache.to(self.device), labels_cache.to(self.device))\n",
        "        epoch_loss = loss.item()\n",
        "\n",
        "        self.model.train()\n",
        "        return epoch_loss, epoch_accuracy, epoch_f1_score, epoch_precision, epoch_recall\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AOGsXO6mS8o"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pdb\n",
        "\n",
        "\n",
        "class biLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embeddings=None, embedding_dim=100, hidden_dim=256, output_dim=3, pad_idx=1, unk_idx=0):\n",
        "        '''\n",
        "        :pad_idx: the index of the padding token <pad> in the vocabulary\n",
        "        :num_layers: number of biLSTMs stacked on top of each other\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        # replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n",
        "        self.embedding.weight.data.copy_(embeddings)\n",
        "        # these are irrelevant for determining sentiment:\n",
        "        self.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "        self.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=2,\n",
        "                           bidirectional=True, dropout=0.5)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        # Note: never use dropout on the input or output layers (text or fc in this case),\n",
        "        # you only ever want to use dropout on intermediate layers.\n",
        "\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "\n",
        "    def forward(self, text, text_lengths):\n",
        "        '''\n",
        "        In some frameworks you must feed the initial hidden state, $h_0$, into the RNN,\n",
        "        however in PyTorch, if no initial hidden state is passed as an argument it defaults to a tensor of all zeros.\n",
        "        :nn.utils.rnn.pack_padded_sequence: This will cause our RNN to only process the non-padded elements of our sequence.\n",
        "        '''\n",
        "        # text : [sent len, batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        # embedded : [sent len, batch size, emb dim]\n",
        "\n",
        "        # pack sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
        "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
        "        # output (packed_output) is the concatenation of the hidden state from every time step\n",
        "        # hidden is simply the final hidden state.\n",
        "        # hidden : [num layers * num directions, batch size, hid dim]\n",
        "        # cell : [num layers * num directions, batch size, hid dim]\n",
        "\n",
        "        # unpack sequence [not needed, only for demonstration]\n",
        "        # output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
        "        # output : [sent len, batch size, hid dim * num directions]\n",
        "        # output over padding tokens are zero tensors\n",
        "\n",
        "        # concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n",
        "        # hidden : [batch size, hid dim * num directions]\n",
        "\n",
        "        return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef6ENk3ZKnBw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pdb\n",
        "\n",
        "\n",
        "class CNN1d(nn.Module):\n",
        "    def __init__(self, vocab_size, embeddings, embedding_dim=200,\n",
        "                 conv_out_ch=200, filter_sizes=[3,4,5], output_dim=3, pad_idx=1, unk_idx=0):\n",
        "        '''\n",
        "        :pad_idx: the index of the padding token <pad> in the vocabulary\n",
        "        :conv_out_ch: number of the different kernels.\n",
        "        :filter_sizes: a list of different kernel sizes we use here.\n",
        "        '''\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
        "        # replace the initial weights of the `embedding` layer with the pre-trained embeddings.\n",
        "        self.embedding.weight.data.copy_(embeddings)\n",
        "        # these are irrelevant for determining sentiment:\n",
        "        self.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
        "        self.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
        "\n",
        "        self.convs = nn.ModuleList([\n",
        "            nn.Conv1d(in_channels=embedding_dim, out_channels=conv_out_ch,\n",
        "                      kernel_size=fs) for fs in filter_sizes])\n",
        "\n",
        "        self.fc = nn.Linear(len(filter_sizes) * conv_out_ch, output_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent len, emb dim]\n",
        "\n",
        "        embedded = embedded.permute(0, 2, 1)\n",
        "        # embedded = [batch size, emb dim, sent len]\n",
        "\n",
        "        # pad if the length of the sentence is less than the kernel size\n",
        "        if embedded.shape[2] < 5:\n",
        "            dif = 5 - embedded.shape[2]\n",
        "            embedded = F.pad(embedded, (0, dif), \"constant\", 0)\n",
        "\n",
        "        conved = [F.relu(conv(embedded)) for conv in self.convs]\n",
        "        # conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "\n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
        "        # pooled_n = [batch size, n_filters]\n",
        "\n",
        "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
        "        # cat = [batch size, n_filters * len(filter_sizes)]\n",
        "\n",
        "        return self.fc(cat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2HJpeoSmTB8"
      },
      "outputs": [],
      "source": [
        "def main_train_postreply():\n",
        "    '''\n",
        "    Main function for training + validation of the second part of the project:\n",
        "    Sentiment analysis of the Post-Replies.\n",
        "    '''\n",
        "    # if we are resuming training on a model\n",
        "    RESUME = False\n",
        "\n",
        "    # Hyper-parameters\n",
        "    NUM_EPOCH = 200 #changed from 500 cause that shit was too long\n",
        "    LOSS_FUNCTION = CrossEntropyLoss\n",
        "    OPTIMIZER = optim.Adam\n",
        "    BATCH_SIZE = 256\n",
        "    MAX_VOCAB_SIZE = 100000 #750000 #max_vocab_size: takes the 100,000 most frequent words as the vocab\n",
        "    lr = 9e-5\n",
        "    optimiser_params = {'lr': lr, 'weight_decay': 1e-4}\n",
        "    EMBEDDING_DIM = 200\n",
        "    HIDDEN_DIM = 300\n",
        "    OUTPUT_DIM = 3\n",
        "    MODEL_MODE = 'CNN'\n",
        "    #MODEL_MODE = \"RNN\" # \"RNN\" or \"CNN\"\n",
        "    conv_out_ch = 200  # for the CNN model:\n",
        "    filter_sizes = [3, 4, 5]  # for the CNN model:\n",
        "    SPLIT_RATIO = 0.9 # ratio of the train set, 1.0 means 100% training, 0% valid data\n",
        "    EXPERIMENT_NAME = \"new_october_CNN\"\n",
        "\n",
        "    #if RESUME == True:\n",
        "    #    params = open_experiment(EXPERIMENT_NAME)\n",
        "    #else:\n",
        "    #    params = create_experiment(EXPERIMENT_NAME)\n",
        "    #cfg_path = params[\"cfg_path\"]\n",
        "\n",
        "    # Prepare data\n",
        "    data_handler = data_provider_PostReply(params=params, batch_size=BATCH_SIZE, split_ratio=SPLIT_RATIO,\n",
        "                                           max_vocab_size=MAX_VOCAB_SIZE, mode=Mode.TRAIN, model_mode=MODEL_MODE)\n",
        "    train_iterator, valid_iterator, vocab_size, PAD_IDX, UNK_IDX, pretrained_embeddings, weights, classes = data_handler.data_loader()\n",
        "\n",
        "    if SPLIT_RATIO == 1:\n",
        "        total_valid_tweets = 0\n",
        "    else:\n",
        "        total_valid_tweets = BATCH_SIZE * len(valid_iterator)\n",
        "    total_train_tweets = BATCH_SIZE * len(train_iterator)\n",
        "    print(f'\\nSummary:\\n----------------------------------------------------')\n",
        "    print(f'Total # of Training tweets: {total_train_tweets:,}')\n",
        "    print(f'Total # of Valid. tweets:   {total_valid_tweets:,}')\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Training(params, num_epochs=NUM_EPOCH, RESUME=RESUME, model_mode=MODEL_MODE)\n",
        "\n",
        "    if MODEL_MODE == \"RNN\":\n",
        "        MODEL = biLSTM(vocab_size=vocab_size, embeddings=pretrained_embeddings, embedding_dim=EMBEDDING_DIM,\n",
        "                       hidden_dim=HIDDEN_DIM, output_dim=OUTPUT_DIM, pad_idx=PAD_IDX, unk_idx=UNK_IDX)\n",
        "    elif MODEL_MODE == \"CNN\":\n",
        "        MODEL = CNN1d(vocab_size=vocab_size, embeddings=pretrained_embeddings, embedding_dim=EMBEDDING_DIM,\n",
        "                       conv_out_ch=conv_out_ch, filter_sizes=filter_sizes, output_dim=OUTPUT_DIM, pad_idx=PAD_IDX, unk_idx=UNK_IDX)\n",
        "\n",
        "    if RESUME == True:\n",
        "        trainer.load_checkpoint(model=MODEL, optimiser=OPTIMIZER,\n",
        "                        optimiser_params=optimiser_params, loss_function=LOSS_FUNCTION, weight=weights)\n",
        "    else:\n",
        "        trainer.setup_model(model=MODEL, optimiser=OPTIMIZER,\n",
        "                        optimiser_params=optimiser_params, loss_function=LOSS_FUNCTION, weight=weights)\n",
        "        # writes the params to config file\n",
        "        #params = read_config(cfg_path)\n",
        "        params['Network']['vocab_size'] = vocab_size\n",
        "        params['Network']['PAD_IDX'] = PAD_IDX\n",
        "        params['Network']['UNK_IDX'] = UNK_IDX\n",
        "        params['Network']['classes'] = classes\n",
        "        params['Network']['SPLIT_RATIO'] = SPLIT_RATIO\n",
        "        params['Network']['MAX_VOCAB_SIZE'] = MAX_VOCAB_SIZE\n",
        "        params['Network']['HIDDEN_DIM'] = HIDDEN_DIM\n",
        "        params['Network']['EMBEDDING_DIM'] = EMBEDDING_DIM\n",
        "        params['Network']['conv_out_ch'] = conv_out_ch\n",
        "        params['Network']['MODEL_MODE'] = MODEL_MODE\n",
        "        params['total_train_tweets'] = total_train_tweets\n",
        "        params['total_valid_tweets'] = total_valid_tweets\n",
        "        #write_config(params, cfg_path, sort_keys=True)\n",
        "\n",
        "    trainer.execute_training(train_loader=train_iterator, valid_loader=valid_iterator, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5uXWcUzgp2rF",
        "outputId": "558edfc9-62bb-4bc8-e58f-706517760b1a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Summary:\n",
            "----------------------------------------------------\n",
            "Total # of Training tweets: 24,064\n",
            "Total # of Valid. tweets:   2,816\n",
            "Total # of model's trainable parameters: 18,168,203\n",
            "----------------------------------------------------\n",
            "\n",
            "Starting time:2022-12-04 15:51:10.726787\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [1/200]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py:652: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [1/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 01 | Epoch Time: 1m 17s | Total Time so far: 1m 17s\n",
            "\tTrain Loss: 1.145 | Train Acc: 35.23% | Train F1: 0.328\n",
            "\t Val. Loss: 1.066 |  Val. Acc: 43.75% |  Val. F1: 0.392\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [2/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [2/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 02 | Epoch Time: 1m 18s | Total Time so far: 2m 35s\n",
            "\tTrain Loss: 1.093 | Train Acc: 39.16% | Train F1: 0.386\n",
            "\t Val. Loss: 1.049 |  Val. Acc: 43.50% |  Val. F1: 0.432\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [3/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [3/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 03 | Epoch Time: 1m 17s | Total Time so far: 3m 53s\n",
            "\tTrain Loss: 1.062 | Train Acc: 41.86% | Train F1: 0.417\n",
            "\t Val. Loss: 1.042 |  Val. Acc: 43.11% |  Val. F1: 0.473\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [4/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [4/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 04 | Epoch Time: 1m 16s | Total Time so far: 5m 10s\n",
            "\tTrain Loss: 1.033 | Train Acc: 44.25% | Train F1: 0.451\n",
            "\t Val. Loss: 1.033 |  Val. Acc: 46.48% |  Val. F1: 0.454\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [5/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [5/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 05 | Epoch Time: 1m 16s | Total Time so far: 6m 26s\n",
            "\tTrain Loss: 1.018 | Train Acc: 45.83% | Train F1: 0.463\n",
            "\t Val. Loss: 1.030 |  Val. Acc: 44.89% |  Val. F1: 0.479\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [6/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [6/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 06 | Epoch Time: 1m 18s | Total Time so far: 7m 45s\n",
            "\tTrain Loss: 0.995 | Train Acc: 47.60% | Train F1: 0.486\n",
            "\t Val. Loss: 1.024 |  Val. Acc: 45.24% |  Val. F1: 0.477\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [7/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [7/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 07 | Epoch Time: 1m 16s | Total Time so far: 9m 1s\n",
            "\tTrain Loss: 0.977 | Train Acc: 49.32% | Train F1: 0.503\n",
            "\t Val. Loss: 1.020 |  Val. Acc: 47.94% |  Val. F1: 0.454\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [8/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [8/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 08 | Epoch Time: 1m 16s | Total Time so far: 10m 18s\n",
            "\tTrain Loss: 0.963 | Train Acc: 50.37% | Train F1: 0.513\n",
            "\t Val. Loss: 1.019 |  Val. Acc: 46.16% |  Val. F1: 0.488\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [9/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [9/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 09 | Epoch Time: 1m 15s | Total Time so far: 11m 34s\n",
            "\tTrain Loss: 0.950 | Train Acc: 51.95% | Train F1: 0.533\n",
            "\t Val. Loss: 1.017 |  Val. Acc: 45.67% |  Val. F1: 0.484\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [10/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [10/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 10 | Epoch Time: 1m 18s | Total Time so far: 12m 53s\n",
            "\tTrain Loss: 0.935 | Train Acc: 52.94% | Train F1: 0.543\n",
            "\t Val. Loss: 1.015 |  Val. Acc: 46.80% |  Val. F1: 0.482\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [11/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [11/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 11 | Epoch Time: 1m 18s | Total Time so far: 14m 12s\n",
            "\tTrain Loss: 0.921 | Train Acc: 54.01% | Train F1: 0.554\n",
            "\t Val. Loss: 1.014 |  Val. Acc: 48.30% |  Val. F1: 0.478\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [12/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [12/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 12 | Epoch Time: 1m 15s | Total Time so far: 15m 28s\n",
            "\tTrain Loss: 0.909 | Train Acc: 55.33% | Train F1: 0.566\n",
            "\t Val. Loss: 1.012 |  Val. Acc: 47.80% |  Val. F1: 0.476\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [13/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [13/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 13 | Epoch Time: 1m 16s | Total Time so far: 16m 44s\n",
            "\tTrain Loss: 0.894 | Train Acc: 56.77% | Train F1: 0.578\n",
            "\t Val. Loss: 1.014 |  Val. Acc: 47.48% |  Val. F1: 0.482\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [14/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [14/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 14 | Epoch Time: 1m 18s | Total Time so far: 18m 3s\n",
            "\tTrain Loss: 0.882 | Train Acc: 57.63% | Train F1: 0.585\n",
            "\t Val. Loss: 1.011 |  Val. Acc: 47.87% |  Val. F1: 0.481\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [15/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [15/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 15 | Epoch Time: 1m 16s | Total Time so far: 19m 19s\n",
            "\tTrain Loss: 0.863 | Train Acc: 58.87% | Train F1: 0.600\n",
            "\t Val. Loss: 1.010 |  Val. Acc: 47.48% |  Val. F1: 0.485\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [16/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [16/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 16 | Epoch Time: 1m 16s | Total Time so far: 20m 36s\n",
            "\tTrain Loss: 0.852 | Train Acc: 59.75% | Train F1: 0.606\n",
            "\t Val. Loss: 1.012 |  Val. Acc: 47.44% |  Val. F1: 0.491\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [17/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [17/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 17 | Epoch Time: 1m 15s | Total Time so far: 21m 51s\n",
            "\tTrain Loss: 0.837 | Train Acc: 61.38% | Train F1: 0.623\n",
            "\t Val. Loss: 1.016 |  Val. Acc: 47.41% |  Val. F1: 0.484\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [18/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [18/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 18 | Epoch Time: 1m 20s | Total Time so far: 23m 12s\n",
            "\tTrain Loss: 0.824 | Train Acc: 61.97% | Train F1: 0.629\n",
            "\t Val. Loss: 1.012 |  Val. Acc: 48.15% |  Val. F1: 0.487\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [19/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [19/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 19 | Epoch Time: 1m 17s | Total Time so far: 24m 29s\n",
            "\tTrain Loss: 0.808 | Train Acc: 63.25% | Train F1: 0.641\n",
            "\t Val. Loss: 1.015 |  Val. Acc: 48.97% |  Val. F1: 0.484\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [20/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [20/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 20 | Epoch Time: 1m 16s | Total Time so far: 25m 46s\n",
            "\tTrain Loss: 0.797 | Train Acc: 64.15% | Train F1: 0.649\n",
            "\t Val. Loss: 1.014 |  Val. Acc: 49.75% |  Val. F1: 0.460\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [21/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [21/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 21 | Epoch Time: 1m 19s | Total Time so far: 27m 5s\n",
            "\tTrain Loss: 0.783 | Train Acc: 65.53% | Train F1: 0.658\n",
            "\t Val. Loss: 1.017 |  Val. Acc: 48.19% |  Val. F1: 0.489\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [22/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [22/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 22 | Epoch Time: 1m 16s | Total Time so far: 28m 22s\n",
            "\tTrain Loss: 0.771 | Train Acc: 66.20% | Train F1: 0.668\n",
            "\t Val. Loss: 1.014 |  Val. Acc: 48.93% |  Val. F1: 0.486\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [23/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [23/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 23 | Epoch Time: 1m 16s | Total Time so far: 29m 39s\n",
            "\tTrain Loss: 0.753 | Train Acc: 67.36% | Train F1: 0.681\n",
            "\t Val. Loss: 1.014 |  Val. Acc: 48.30% |  Val. F1: 0.483\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [24/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [24/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 24 | Epoch Time: 1m 17s | Total Time so far: 30m 56s\n",
            "\tTrain Loss: 0.739 | Train Acc: 68.20% | Train F1: 0.689\n",
            "\t Val. Loss: 1.018 |  Val. Acc: 48.97% |  Val. F1: 0.482\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [25/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [25/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 25 | Epoch Time: 1m 19s | Total Time so far: 32m 16s\n",
            "\tTrain Loss: 0.727 | Train Acc: 69.06% | Train F1: 0.696\n",
            "\t Val. Loss: 1.018 |  Val. Acc: 48.79% |  Val. F1: 0.478\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [26/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [26/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 26 | Epoch Time: 1m 16s | Total Time so far: 33m 32s\n",
            "\tTrain Loss: 0.716 | Train Acc: 69.76% | Train F1: 0.702\n",
            "\t Val. Loss: 1.020 |  Val. Acc: 49.08% |  Val. F1: 0.483\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [27/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [27/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 27 | Epoch Time: 1m 16s | Total Time so far: 34m 49s\n",
            "\tTrain Loss: 0.702 | Train Acc: 70.84% | Train F1: 0.711\n",
            "\t Val. Loss: 1.022 |  Val. Acc: 49.25% |  Val. F1: 0.470\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [28/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [28/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 28 | Epoch Time: 1m 16s | Total Time so far: 36m 5s\n",
            "\tTrain Loss: 0.688 | Train Acc: 71.70% | Train F1: 0.722\n",
            "\t Val. Loss: 1.022 |  Val. Acc: 48.37% |  Val. F1: 0.476\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [29/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [29/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 29 | Epoch Time: 1m 18s | Total Time so far: 37m 24s\n",
            "\tTrain Loss: 0.675 | Train Acc: 72.77% | Train F1: 0.732\n",
            "\t Val. Loss: 1.023 |  Val. Acc: 48.37% |  Val. F1: 0.485\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [30/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [30/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 30 | Epoch Time: 1m 16s | Total Time so far: 38m 41s\n",
            "\tTrain Loss: 0.658 | Train Acc: 73.78% | Train F1: 0.743\n",
            "\t Val. Loss: 1.027 |  Val. Acc: 50.53% |  Val. F1: 0.460\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [31/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [31/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 31 | Epoch Time: 1m 16s | Total Time so far: 39m 58s\n",
            "\tTrain Loss: 0.650 | Train Acc: 73.94% | Train F1: 0.742\n",
            "\t Val. Loss: 1.028 |  Val. Acc: 50.36% |  Val. F1: 0.466\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [32/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [32/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 32 | Epoch Time: 1m 18s | Total Time so far: 41m 16s\n",
            "\tTrain Loss: 0.637 | Train Acc: 74.80% | Train F1: 0.751\n",
            "\t Val. Loss: 1.028 |  Val. Acc: 49.47% |  Val. F1: 0.471\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [33/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [33/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 33 | Epoch Time: 1m 16s | Total Time so far: 42m 33s\n",
            "\tTrain Loss: 0.624 | Train Acc: 76.06% | Train F1: 0.762\n",
            "\t Val. Loss: 1.034 |  Val. Acc: 49.11% |  Val. F1: 0.475\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [34/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [34/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 34 | Epoch Time: 1m 16s | Total Time so far: 43m 49s\n",
            "\tTrain Loss: 0.616 | Train Acc: 75.85% | Train F1: 0.762\n",
            "\t Val. Loss: 1.037 |  Val. Acc: 50.53% |  Val. F1: 0.461\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [35/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [35/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 35 | Epoch Time: 1m 16s | Total Time so far: 45m 6s\n",
            "\tTrain Loss: 0.606 | Train Acc: 76.65% | Train F1: 0.769\n",
            "\t Val. Loss: 1.036 |  Val. Acc: 49.79% |  Val. F1: 0.465\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [36/200]\n",
            "\n",
            "Validation (intermediate metrics):\n",
            "Epoch [36/200]\n",
            "\n",
            "---------------------------------------------------------------\n",
            "Epoch: 36 | Epoch Time: 1m 18s | Total Time so far: 46m 25s\n",
            "\tTrain Loss: 0.594 | Train Acc: 77.09% | Train F1: 0.772\n",
            "\t Val. Loss: 1.040 |  Val. Acc: 49.86% |  Val. F1: 0.464\n",
            "---------------------------------------------------------------\n",
            "\n",
            "Training (intermediate metrics):\n",
            "Epoch [37/200]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-39e6b7bb1e7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_train_postreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-dbbe77ce89df>\u001b[0m in \u001b[0;36mmain_train_postreply\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m#write_config(params, cfg_path, sort_keys=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-8ae0bfe81925>\u001b[0m in \u001b[0;36mexecute_training\u001b[0;34m(self, train_loader, valid_loader, batch_size)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training (intermediate metrics):'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_F1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_recall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_precision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-8ae0bfe81925>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, train_loader, batch_size)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;31m# Prints loss statistics after number of steps specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    108\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "main_train_postreply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2GhLgkuxJ9c"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import ParamSpecArgs\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "\n",
        "class Prediction:\n",
        "    '''\n",
        "    This class represents prediction (testing) process similar to the Training class.\n",
        "    '''\n",
        "    def __init__(self, params, classes, model_mode='RNN', cfg_path_RNN=None, cfg_path_CNN=None): #cfg_path\n",
        "        self.params = params\n",
        "        #if cfg_path_CNN:\n",
        "            #self.params_RNN = read_config(cfg_path_RNN)\n",
        "            #self.params_CNN = read_config(cfg_path_CNN)\n",
        "        #self.cfg_path = cfg_path\n",
        "        self.setup_cuda()\n",
        "        self.model_mode = model_mode\n",
        "        self.classes = classes\n",
        "\n",
        "    def setup_cuda(self, cuda_device_id=0):\n",
        "        if torch.cuda.is_available():\n",
        "            torch.backends.cudnn.fastest = True\n",
        "            torch.cuda.set_device(cuda_device_id)\n",
        "            self.device = torch.device('cuda')\n",
        "        else:\n",
        "            self.device = torch.device('cpu')\n",
        "            #map_location=torch.device('cpu')\n",
        "\n",
        "\n",
        "    def epoch_time(self, start_time, end_time):\n",
        "        elapsed_time = end_time - start_time\n",
        "        elapsed_mins = int(elapsed_time / 60)\n",
        "        elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "        return elapsed_mins, elapsed_secs\n",
        "\n",
        "\n",
        "    def setup_model(self, model, vocab_size, embeddings, embedding_dim,\n",
        "                    hidden_dim, pad_idx, unk_idx, model_file_name=None, epoch=19,\n",
        "                    conv_out_ch=200, filter_sizes=[3,4,5], model_c =CNN1d, model_r=biLSTM):\n",
        "        if model_file_name == None:\n",
        "            model_file_name = self.params['trained_model_name']\n",
        "        if self.model_mode == \"RNN\":\n",
        "            self.model_p = model(vocab_size=vocab_size, embeddings=embeddings, embedding_dim=embedding_dim,\n",
        "                                 hidden_dim=hidden_dim, pad_idx=pad_idx, unk_idx=unk_idx).to(self.device)\n",
        "        elif self.model_mode == \"CNN\":\n",
        "            self.model_p = model(vocab_size=vocab_size, embeddings=embeddings, embedding_dim=embedding_dim,\n",
        "                                 conv_out_ch=conv_out_ch, filter_sizes=filter_sizes, pad_idx=pad_idx, unk_idx=unk_idx).to(self.device)\n",
        "        elif self.model_mode == \"ensemble\":\n",
        "            model_file_name_c = self.params_CNN['trained_model_name']\n",
        "            model_file_name_r = self.params_RNN['trained_model_name']\n",
        "            self.model_cnn = model_c(vocab_size=vocab_size, embeddings=embeddings, embedding_dim=embedding_dim,\n",
        "                                 conv_out_ch=conv_out_ch, filter_sizes=filter_sizes, pad_idx=pad_idx, unk_idx=unk_idx).to(self.device)\n",
        "            self.model_rnn = model_r(vocab_size=vocab_size, embeddings=embeddings, embedding_dim=embedding_dim,\n",
        "                                 hidden_dim=hidden_dim, pad_idx=pad_idx, unk_idx=unk_idx).to(self.device)\n",
        "\n",
        "        # Loads model from model_file_name and default network_output_path\n",
        "        if self.model_mode == \"ensemble\":\n",
        "            # self.model_cnn.load_state_dict(torch.load(self.params_CNN['network_output_path'] + \"/\" + model_file_name_c))\n",
        "            self.model_cnn.load_state_dict(\n",
        "                torch.load(self.params_CNN['network_output_path'] + model_file_name_c)) #\"/epoch\" + str(19) + \"_\" + model_file_name_c))\n",
        "            # self.model_rnn.load_state_dict(torch.load(self.params_RNN['network_output_path'] + \"/\" + model_file_name_r))\n",
        "            self.model_rnn.load_state_dict(\n",
        "                torch.load(self.params_RNN['network_output_path'] + model_file_name_r)) #\"/epoch\" + str(43) + \"_\" + model_file_name_r))\n",
        "        else:\n",
        "            # self.model_p.load_state_dict(torch.load(self.params['network_output_path'] + \"/\" + model_file_name))\n",
        "            self.model_p.load_state_dict(torch.load(self.params['network_output_path'] + model_file_name, map_location=torch.device('cpu') )) #+ \"/epoch\" + str(epoch) + \"_\" + model_file_name))\n",
        "\n",
        "\n",
        "    def predict(self, test_loader, batch_size):\n",
        "        # Reads params to check if any params have been changed by user\n",
        "        #self.params = read_config(self.cfg_path)\n",
        "        self.model_p.eval()\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            # initializing the caches\n",
        "            logits_cache = torch.from_numpy(np.zeros((len(test_loader) * batch_size, 3)))\n",
        "            max_preds_cache = torch.from_numpy(np.zeros((len(test_loader) * batch_size, 1)))\n",
        "            labels_cache = torch.from_numpy(np.zeros(len(test_loader) * batch_size))\n",
        "\n",
        "            for idx, batch in enumerate(test_loader):\n",
        "                if self.model_mode == \"RNN\":\n",
        "                    message, message_lengths = batch.text\n",
        "                if self.model_mode == \"CNN\":\n",
        "                    message = batch.text\n",
        "                label = batch.label\n",
        "                message = message.long()\n",
        "                label = label.long()\n",
        "                message = message.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                if self.model_mode == \"RNN\":\n",
        "                    output = self.model_p(message, message_lengths).squeeze(1)\n",
        "                if self.model_mode == \"CNN\":\n",
        "                    output = self.model_p(message).squeeze(1)\n",
        "                max_preds = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "\n",
        "                # saving the logits and labels of this batch\n",
        "                for i, batch_vector in enumerate(max_preds):\n",
        "                    max_preds_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, batch_vector in enumerate(output):\n",
        "                    logits_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, value in enumerate(label):\n",
        "                    labels_cache[idx * batch_size + i] = value\n",
        "\n",
        "        '''Metrics calculation over the whole set'''\n",
        "        max_preds_cache = max_preds_cache.cpu()\n",
        "        labels_cache = labels_cache.cpu()\n",
        "\n",
        "        # average=None gives individual scores for each class\n",
        "        # here we only care about the average of positive class and negative class\n",
        "        final_accuracy = metrics.accuracy_score(labels_cache, max_preds_cache)\n",
        "        # final_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # final_precision = metrics.precision_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # final_recall = metrics.recall_score(labels_cache, max_preds_cache, average='macro')\n",
        "\n",
        "        final_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average=None)\n",
        "        final_precision = metrics.precision_score(labels_cache, max_preds_cache, average=None)\n",
        "        final_recall = metrics.recall_score(labels_cache, max_preds_cache, average=None)\n",
        "        final_f1_score = (final_f1_score[1] + final_f1_score[2]) / 2\n",
        "        final_precision = (final_precision[1] + final_precision[2]) / 2\n",
        "        final_recall = (final_recall[1] + final_recall[2]) / 2\n",
        "        confusion_matrix = metrics.confusion_matrix(labels_cache, max_preds_cache, labels=[0,1,2])\n",
        "\n",
        "        end_time = time.time()\n",
        "        test_mins, test_secs = self.epoch_time(start_time, end_time)\n",
        "\n",
        "        # Print the final evaluation metrics\n",
        "        print('\\n----------------------------------------------------------------------')\n",
        "        print(f'Testing | Testing Time: {test_mins}m {test_secs}s')\n",
        "        print(f'\\tAcc: {final_accuracy * 100:.2f}% | F1 score: {final_f1_score:.3f} | '\n",
        "              f'Recall: {final_recall:.3f} | Precision: {final_precision:.3f}')\n",
        "        print('----------------------------------------------------------------------\\n')\n",
        "        print(confusion_matrix)\n",
        "        # self.plot_confusion_matrix(confusion_matrix, target_names=self.classes,\n",
        "        #                       title='Confusion matrix, without normalization')\n",
        "        return final_accuracy, final_f1_score\n",
        "\n",
        "\n",
        "    def predict_ensemble(self, test_iterator_RNN, test_iterator_CNN, batch_size):\n",
        "        \"prediction with ensembling CNN and RNN outputs by normal averaging\"\n",
        "\n",
        "        # Reads params to check if any params have been changed by user\n",
        "        #self.params = read_config(self.cfg_path)\n",
        "        self.model_cnn.eval()\n",
        "        self.model_rnn.eval()\n",
        "\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            # initializing the caches\n",
        "            logits_cache = torch.from_numpy(np.zeros((len(test_iterator_RNN) * batch_size, 3)))\n",
        "            max_preds_cache = torch.from_numpy(np.zeros((len(test_iterator_RNN) * batch_size, 1)))\n",
        "            labels_cache = torch.from_numpy(np.zeros(len(test_iterator_RNN) * batch_size))\n",
        "\n",
        "            for idx, (batch_RNN, batch_CNN) in enumerate(zip(test_iterator_RNN, test_iterator_CNN)):\n",
        "\n",
        "                # RNN part\n",
        "                message, message_lengths = batch_RNN.text\n",
        "                label = batch_RNN.label\n",
        "                message = message.long()\n",
        "                label = label.long()\n",
        "                message = message.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                output_RNN = self.model_rnn(message, message_lengths).squeeze(1)\n",
        "\n",
        "                #CNN part\n",
        "                message = batch_CNN.text\n",
        "                label = batch_CNN.label\n",
        "                message = message.long()\n",
        "                label = label.long()\n",
        "                message = message.to(self.device)\n",
        "                label = label.to(self.device)\n",
        "                output_CNN = self.model_cnn(message).squeeze(1)\n",
        "\n",
        "                output = (output_CNN + output_RNN) / 2\n",
        "                max_preds = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
        "                # saving the logits and labels of this batch\n",
        "                for i, batch_vector in enumerate(max_preds):\n",
        "                    max_preds_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, batch_vector in enumerate(output):\n",
        "                    logits_cache[idx * batch_size + i] = batch_vector\n",
        "                for i, value in enumerate(label):\n",
        "                    labels_cache[idx * batch_size + i] = value\n",
        "\n",
        "        '''Metrics calculation over the whole set'''\n",
        "        max_preds_cache = max_preds_cache.cpu()\n",
        "        labels_cache = labels_cache.cpu()\n",
        "\n",
        "        # average=None gives individual scores for each class\n",
        "        # here we only care about the average of positive class and negative class\n",
        "        final_accuracy = metrics.accuracy_score(labels_cache, max_preds_cache)\n",
        "        # final_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # final_precision = metrics.precision_score(labels_cache, max_preds_cache, average='macro')\n",
        "        # final_recall = metrics.recall_score(labels_cache, max_preds_cache, average='macro')\n",
        "\n",
        "        final_f1_score = metrics.f1_score(labels_cache, max_preds_cache, average=None)\n",
        "        final_precision = metrics.precision_score(labels_cache, max_preds_cache, average=None)\n",
        "        final_recall = metrics.recall_score(labels_cache, max_preds_cache, average=None)\n",
        "        final_f1_score = (final_f1_score[1] + final_f1_score[2]) / 2\n",
        "        final_precision = (final_precision[1] + final_precision[2]) / 2\n",
        "        final_recall = (final_recall[1] + final_recall[2]) / 2\n",
        "        confusion_matrix = metrics.confusion_matrix(labels_cache, max_preds_cache, labels=[0,1,2])\n",
        "\n",
        "        end_time = time.time()\n",
        "        test_mins, test_secs = self.epoch_time(start_time, end_time)\n",
        "\n",
        "        # Print the final evaluation metrics\n",
        "        print('\\n----------------------------------------------------------------------')\n",
        "        print(f'Testing | Testing Time: {test_mins}m {test_secs}s')\n",
        "        print(f'\\tAcc: {final_accuracy * 100:.2f}% | F1 score: {final_f1_score:.3f} | '\n",
        "              f'Recall: {final_recall:.3f} | Precision: {final_precision:.3f}')\n",
        "        print('----------------------------------------------------------------------\\n')\n",
        "        print(confusion_matrix)\n",
        "        # self.plot_confusion_matrix(confusion_matrix, target_names=self.classes,\n",
        "        #                       title='Confusion matrix, without normalization')\n",
        "        return final_accuracy, final_f1_score\n",
        "\n",
        "\n",
        "    def plot_confusion_matrix(self, cm, target_names,\n",
        "                              title='Confusion matrix', cmap=None, normalize=False):\n",
        "        \"\"\"\n",
        "        given a sklearn confusion matrix (cm), make a nice plot\n",
        "        ---------\n",
        "        cm:           confusion matrix from sklearn.metrics.confusion_matrix\n",
        "        target_names: given classification classes such as [0, 1, 2]\n",
        "                      the class names, for example: ['high', 'medium', 'low']\n",
        "        cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n",
        "                      plt.get_cmap('jet') or plt.cm.Blues\n",
        "        normalize:    If False, plot the raw numbers\n",
        "                      If True, plot the proportions\n",
        "        \"\"\"\n",
        "        accuracy = np.trace(cm) / np.sum(cm).astype('float')\n",
        "        misclass = 1 - accuracy\n",
        "\n",
        "        if cmap is None:\n",
        "            cmap = plt.get_cmap('Blues')\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "        plt.title(title)\n",
        "        plt.colorbar()\n",
        "\n",
        "        if target_names is not None:\n",
        "            tick_marks = np.arange(len(target_names))\n",
        "            plt.xticks(tick_marks, target_names, rotation=45)\n",
        "            plt.yticks(tick_marks, target_names)\n",
        "\n",
        "        if normalize:\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "        thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "        for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "            if normalize:\n",
        "                plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "            else:\n",
        "                plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                         horizontalalignment=\"center\",\n",
        "                         color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        plt.tight_layout()\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label\\naccuracy={:0.2f}%; misclass={:0.2f}%'.format(accuracy*100, misclass*100))\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def manual_predict(self, labels, vocab_idx, phrase, min_len=4,\n",
        "                       tokenizer=spacy.load(\"en_core_web_sm\"), mode=None, prediction_mode='Manualpart1'):\n",
        "        '''\n",
        "        Manually predicts the polarity of the given sentence.\n",
        "        Possible polarities: 1.neutral, 2.positive, 3.negative\n",
        "        '''\n",
        "        #self.params = read_config(self.cfg_path)\n",
        "        self.model_p.eval()\n",
        "\n",
        "        tokenized = [tok.text for tok in tokenizer.tokenizer(phrase)]\n",
        "        if len(tokenized) < min_len:\n",
        "            tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
        "        indexed = [vocab_idx[t] for t in tokenized]\n",
        "        tensor = torch.LongTensor(indexed).to(self.device)\n",
        "        tensor = tensor.unsqueeze(1)\n",
        "        preds = self.model_p(tensor, torch.Tensor([tensor.shape[0]]))\n",
        "        max_preds = preds.argmax(dim=1)\n",
        "        if mode == Mode.REPLYPREDICTION:\n",
        "            return labels[max_preds.item()]\n",
        "\n",
        "        print('\\n\\t', '\"' + phrase + '\"')\n",
        "        print('-----------------------------------------')\n",
        "        if prediction_mode == 'Manualpart1':\n",
        "            print(f'\\t This is a {labels[max_preds.item()]} phrase!')\n",
        "        elif prediction_mode == 'Manualpart2':\n",
        "            print(f'\\t This phrase is likely to get {labels[max_preds.item()]} replies!')\n",
        "        print('-----------------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ddUu_8mxoPP"
      },
      "outputs": [],
      "source": [
        "def main_test_postreply():\n",
        "    '''Main function for testing of the second part of the project\n",
        "    Sentiment analysis of the Post-Replies.\n",
        "    '''\n",
        "    EXPERIMENT_NAME = 'new_october_CNN'\n",
        "    BATCH_SIZE = 256\n",
        "\n",
        "    #params = open_experiment(EXPERIMENT_NAME)\n",
        "    #cfg_path = params['cfg_path']\n",
        "    vocab_size = params['Network']['vocab_size']\n",
        "    PAD_IDX = params['Network']['PAD_IDX']\n",
        "    UNK_IDX = params['Network']['UNK_IDX']\n",
        "    classes = params['Network']['classes']\n",
        "    MAX_VOCAB_SIZE = params['Network']['MAX_VOCAB_SIZE']\n",
        "    SPLIT_RATIO = params['Network']['SPLIT_RATIO']\n",
        "    EMBEDDING_DIM = params['Network']['EMBEDDING_DIM']\n",
        "    HIDDEN_DIM = params['Network']['HIDDEN_DIM']\n",
        "    conv_out_ch = params['Network']['conv_out_ch']\n",
        "    MODEL_MODE = params['Network']['MODEL_MODE']\n",
        "    pretrained_embeddings = torch.zeros((vocab_size, EMBEDDING_DIM))\n",
        "\n",
        "    # Prepare data\n",
        "    data_handler_test = data_provider_PostReply(params=params, batch_size=BATCH_SIZE, split_ratio=SPLIT_RATIO,\n",
        "                                         max_vocab_size=MAX_VOCAB_SIZE, mode=Mode.TEST, model_mode=MODEL_MODE)\n",
        "    test_iterator = data_handler_test.data_loader()\n",
        "    # Initialize predictor\n",
        "    predictor = Prediction(params, model_mode=MODEL_MODE, classes=classes) #cfg_path\n",
        "\n",
        "    if MODEL_MODE == \"RNN\":\n",
        "        MODEL = biLSTM\n",
        "    elif MODEL_MODE == \"CNN\":\n",
        "        MODEL = CNN1d\n",
        "\n",
        "    predictor.setup_model(model=MODEL, vocab_size=vocab_size, embeddings=pretrained_embeddings,\n",
        "                          embedding_dim=EMBEDDING_DIM, hidden_dim=HIDDEN_DIM, pad_idx=PAD_IDX, unk_idx=UNK_IDX,\n",
        "                          conv_out_ch=conv_out_ch, filter_sizes=[3, 4, 5])\n",
        "    predictor.predict(test_iterator, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQV6lQRqCNyW"
      },
      "outputs": [],
      "source": [
        "\n",
        " params={'Network': {'seed': 1,\n",
        "  'total_param_num': 18168203,\n",
        "  'optimiser': 'Adam',\n",
        "  'loss_function': 'CrossEntropyLoss',\n",
        "  'optimiser_params': {'lr': 9e-05, 'weight_decay': 0.0001},\n",
        "  'vocab_size': 81423,\n",
        "  'PAD_IDX': 1,\n",
        "  'UNK_IDX': 0,\n",
        "  'classes': ['neutral', 'negative', 'positive'],\n",
        "  'SPLIT_RATIO': 0.9,\n",
        "  'MAX_VOCAB_SIZE': 100000,\n",
        "  'HIDDEN_DIM': 300,\n",
        "  'EMBEDDING_DIM': 200,\n",
        "  'conv_out_ch': 200,\n",
        "  'MODEL_MODE': 'CNN',\n",
        "  'num_epoch': 200},\n",
        " 'display_stats_freq': 200,\n",
        " 'network_save_freq': 1,\n",
        " 'postreply_data_path': './',\n",
        " 'final_data_post_reply_file_name': 'final_data_post_reply.csv',\n",
        " 'training_post_reply_file_name': 'obtained_train.csv',\n",
        " 'final_test_post_reply_file_name': 'test_w_text.csv',\n",
        " 'reply_data_format': 'csv',\n",
        " 'pretrained_embedding': 'glove.twitter.27B.200d',\n",
        " 'tokenizer': 'spacy',\n",
        " 'network_output_path': './models/',\n",
        " 'trained_model_name': 'trained_CNN_obt.pth',\n",
        " 'total_train_tweets': 24064,\n",
        " 'total_valid_tweets': 2816}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 987
        },
        "id": "5M2zL5BIxplN",
        "outputId": "5738bbe8-0ee6-4612-c6a7-58a1255dffca"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e044dbe832bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain_test_postreply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-9e22aa1c701d>\u001b[0m in \u001b[0;36mmain_test_postreply\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m                           \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDING_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mHIDDEN_DIM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPAD_IDX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUNK_IDX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                           conv_out_ch=conv_out_ch, filter_sizes=[3, 4, 5])\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-9ff14558e7fb>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, test_loader, batch_size)\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_mode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CNN\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mmax_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get the index of the max probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-f580a782195a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# text = [batch size, sent len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;31m# embedded = [batch size, sent len, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "main_test_postreply()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}